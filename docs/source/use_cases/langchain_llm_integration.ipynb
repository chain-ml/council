{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Langchain LLM integration\n",
    "\n",
    "In this notebook, we show how it's easy to build **Council** agents that leverage the power of **Langchain** to access a wide variety of LLMs.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Integration with **Langchain** is easy and straightforward.\n",
    "To use **Langchain** with the **Council** framework, you will need to add the extras \"langchain\" dependency when installing it via pip.\n",
    "\n",
    "### Example\n",
    "\n",
    "```sh\n",
    "$ pip install council[langchain]\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build a `LangchainLLM` to integrate langchain llm into ChainML."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "from council.llm import LLMBase\n",
    "from council.llm.llm_message import LLMMessage\n",
    "\n",
    "from langchain.llms import BaseLLM\n",
    "\n",
    "\n",
    "class LangChainLLM(LLMBase):\n",
    "    langchain_llm: BaseLLM\n",
    "\n",
    "    def __init__(self, langchain_llm: BaseLLM):\n",
    "        super().__init__()\n",
    "        self.langchain_llm = langchain_llm\n",
    "\n",
    "    def _post_chat_request(self, messages: list[LLMMessage], **kwargs: Any) -> List[str]:\n",
    "        prompt = messages[-1].content\n",
    "        return [self.langchain_llm.__call__(prompt=prompt, **kwargs)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examples\n",
    "### HuggingFace pipeline\n",
    "\n",
    "Let's create a langchain LLM using the `HuggingFacePipeline`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline.from_model_id(model_id=\"google/flan-t5-large\", task=\"text2text-generation\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wrap the langchain LLM into our newly created `LangchainLLM`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hugging_face_llm = LangChainLLM(langchain_llm=hf_pipeline)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `ChainML` LLM is now ready to use!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = LLMMessage.user_message(\"Tell me more about blockchains\")\n",
    "hugging_face_llm.post_chat_request(messages=[prompt])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And that's it! Your langchain LLM is now ready to be used in the `ChainML` framework!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### OpenAI chat model\n",
    "\n",
    "Let's build a `LangchainChatLLM` to integrate langchain chat llm into ChainML."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from council.llm.llm_message import LLMMessageRole\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.schema.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "\n",
    "class LangChainChatLLM(LLMBase):\n",
    "    langchain_llm: BaseChatModel\n",
    "\n",
    "    def __init__(self, langchain_llm: BaseChatModel):\n",
    "        super().__init__()\n",
    "        self.langchain_llm = langchain_llm\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_message(message: LLMMessage) -> BaseMessage:\n",
    "        if message.is_of_role(LLMMessageRole.User):\n",
    "            return HumanMessage(content=message.content)\n",
    "        elif message.is_of_role(LLMMessageRole.System):\n",
    "            return SystemMessage(content=message.content)\n",
    "        elif message.is_of_role(LLMMessageRole.Assistant):\n",
    "            return AIMessage(content=message.content)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid role {message.role}\")\n",
    "\n",
    "    def _post_chat_request(self, messages: list[LLMMessage], **kwargs: Any) -> List[str]:\n",
    "        messages = map(lambda msg: LangChainChatLLM.convert_message(msg), messages)\n",
    "        return [self.langchain_llm(messages=list(messages), **kwargs).content]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create a langchain chat llm using the `ChatOpenAI`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "lc_chatgpt = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Wrap `ChatOpenAI` into our newly created `LangChainChatLLM`\n",
    "chatgpt_llm = LangChainChatLLM(lc_chatgpt)\n",
    "\n",
    "# Build history of messages\n",
    "messages = [\n",
    "    LLMMessage.system_message(\n",
    "        \"You are a helpful assistant from times of olde. Always answer using Shakespearian english.\"\n",
    "    ),\n",
    "    LLMMessage.user_message(\"What is the continent to the South of Mexico?\"),\n",
    "    LLMMessage.assistant_message(\"Behold methinks it be South America\"),\n",
    "    LLMMessage.user_message(\"what are the three largest cities in that continent?\"),\n",
    "]\n",
    "\n",
    "# Call the model\n",
    "result = chatgpt_llm.post_chat_request(messages)\n",
    "print(result, end=\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
