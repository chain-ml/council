{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LlamaIndex integration\n",
    "\n",
    "In this notebook, we show how it's easy to build **ChainML** agents that leverage the power of **LlamaIndex** to integrate data for your agents.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Integration with **LlamaIndex** is easy and straightforward.\n",
    "To use **LlamaIndex** with the **ChainML** framework, you will need to install \"Llama-Index\" via pip.\n",
    "\n",
    "### Example\n",
    "\n",
    "```sh\n",
    "$ pip install Llama-Index\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Example of using Llama Index to retrieve relevant information from documents - here we use the book, the Great Gatbsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download Great Gatsby example from Llama Index\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "url = \"https://github.com/jerryjliu/llama_index/blob/main/examples/gatsby/gatsby_full.txt\"\n",
    "filename = url.split(\"/\")[-1]\n",
    "\n",
    "os.makedirs(\"gatsby_download\", exist_ok=True)\n",
    "\n",
    "response = requests.get(url)\n",
    "with open(os.path.join(\"gatsby_download\", filename), \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# build index of book\n",
    "documents = SimpleDirectoryReader(\"gatsby_download\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gatsby and Daisy meet at Gatsby's house.\n"
     ]
    }
   ],
   "source": [
    "# check the index is working\n",
    "response = query_engine.query(\"Where do Gatsby and Daisy meet?\")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Examples\n",
    "### Chain Using LlamaIndex\n",
    "\n",
    "Let's create a chain that uses LlamaIndex to retrieve relevant context for a user's query for a simple chatbot that can query the data. We will look up context then feed it into a prompt template for an LLMSkill to respond.\n",
    "\n",
    "We define a LlamaIndexSkill that uses a query engine to look up indexed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainml_engine.core import SkillBase, Budget\n",
    "from chainml_engine.core.execution_context import SkillContext, SkillMessage\n",
    "\n",
    "from llama_index.indices.query.base import BaseQueryEngine\n",
    "\n",
    "\n",
    "class LlamaIndexSkill(SkillBase):\n",
    "    queryEngine: BaseQueryEngine\n",
    "\n",
    "    def __init__(self, queryEngine: BaseQueryEngine):\n",
    "        SkillBase.__init__(self, \"llama index skill\")\n",
    "        self.queryEngine = queryEngine\n",
    "\n",
    "    def execute(self, context: SkillContext, budget: Budget) -> SkillMessage:\n",
    "        prompt = context.chatHistory.last_user_message().unwrap(\"no user message\").message\n",
    "        print(prompt)\n",
    "        response = self.queryEngine.query(prompt)\n",
    "        return self.build_success_message(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap into a trivial agent that just answers document queries\n",
    "from chainml_engine.skill import LLMSkill\n",
    "from chainml_engine.core import Chain, Agent\n",
    "from chainml_engine.controller.basic_controller import BasicController\n",
    "from chainml_engine.evaluator.basic_evaluator import BasicEvaluator\n",
    "\n",
    "index_skill = LlamaIndexSkill(query_engine)\n",
    "chain = Chain(\"docindex\", \"document index\", [index_skill])\n",
    "agent = Agent(BasicController(), [chain], BasicEvaluator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who falls in love with Daisy?\n",
      "\n",
      "Gatsby.\n"
     ]
    }
   ],
   "source": [
    "from chainml_engine.llm.llm_message import LLMMessage\n",
    "from chainml_engine.core.execution_context import AgentContext, ChatHistory\n",
    "\n",
    "chat_history = ChatHistory()\n",
    "# chat_history.add_user_message(message=\"Whose eyes are on the billboard?\")\n",
    "# chat_history.add_user_message(\"What are the personalities of Tom and Daisy?\")\n",
    "# chat_history.add_user_message(\"What era does the book take place in?\")\n",
    "chat_history.add_user_message(\"Who falls in love with Daisy?\")\n",
    "context = AgentContext(chat_history=chat_history)\n",
    "result = agent.execute(context=context, budget=Budget(20))\n",
    "print(result.messages[-1].message.message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of integrating LlamaIndex into a more capable chain - this chain answers a user request for information about the book by:\n",
    "1. formulating a query for information to retrieve from the book using an LLM (here GPT-3.5-Turbo)\n",
    "2. passing that query to LlamaIndex to retrieve relevant passages from the book  \n",
    "3. passing those passages as context to the LLM along with the original query to generate a summary response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent to use index to provide context for more complex answers\n",
    "from chainml_engine.llm import OpenAILLM, OpenAIConfiguration\n",
    "import dotenv\n",
    "from chainml_engine.prompt import PromptBuilder\n",
    "from chainml_engine.prompt.prompt_builder import PromptToMessages\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "config = OpenAIConfiguration.from_env()\n",
    "config.model = \"gpt-3.5-turbo\"\n",
    "llm = OpenAILLM(config)\n",
    "\n",
    "context_prompt = PromptToMessages(\n",
    "    PromptBuilder(\"Please identify query terms to respond to the following user request {{chat_history.last_message}}\")\n",
    ")\n",
    "context_query_skill = LLMSkill(\n",
    "    llm,\n",
    "    \"You are an expert in the Great Gatbsy. Identify relevant query terms to search for context in the book.\",\n",
    "    context_messages=context_prompt.get_messages_from_prompt,\n",
    ")\n",
    "\n",
    "index_skill = LlamaIndexSkill(query_engine)\n",
    "index_prompt = PromptToMessages(\n",
    "    PromptBuilder(\n",
    "        \"Here are relevant quotes from the book: {{chain_history.last_message}} \\nUse this to respond to the following user request {{chat_history.last_message}}\"\n",
    "    )\n",
    ")\n",
    "response_skill = LLMSkill(\n",
    "    llm,\n",
    "    \"You are an expert in the Great Gatbsy. Provide a helpful response to the user's question\",\n",
    "    context_messages=index_prompt.get_messages_from_prompt,\n",
    ")\n",
    "\n",
    "chain = Chain(\"docindex\", \"document index\", [context_query_skill, index_skill, response_skill])\n",
    "agent = Agent(BasicController(), [chain], BasicEvaluator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"[%(asctime)s %(levelname)s %(threadName)s %(name)s:%(funcName)s:%(lineno)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S%z\",\n",
    ")\n",
    "## uncomment me to see the engine logs\n",
    "logging.getLogger(\"chainml_engine\").setLevel(logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the key plot events in the book?\n",
      "The key plot events in The Great Gatsby include a man named Jimmy making a list of resolutions to better himself, the narrator meeting Tom Buchanan's mistress in the Valley of Ashes, the narrator attending Gatsby's funeral, and the narrator and the owl-eyed man discussing Gatsby's death. These events play a significant role in developing the story and character dynamics in the novel. Let me know if you have any more questions!\n"
     ]
    }
   ],
   "source": [
    "chat_history = ChatHistory()\n",
    "# chat_history.add_user_message(message=\"Whose eyes are on the billboard?\")\n",
    "# chat_history.add_user_message(\"What are the personalities of Tom and Daisy?\")\n",
    "# chat_history.add_user_message(\"What era does the book take place in?\")\n",
    "chat_history.add_user_message(\"What are the key plot events in the book?\")\n",
    "# chat_history.add_user_message(\"What is the significance of the eyes on the billboard?\")\n",
    "context = AgentContext(chat_history=chat_history)\n",
    "result = agent.execute(context=context, budget=Budget(20))\n",
    "print(result.messages[-1].message.message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
